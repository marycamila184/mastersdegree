{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>cls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.45860</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.92420</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.01120</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.57180</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.36840</td>\n",
       "      <td>9.6718</td>\n",
       "      <td>-3.96060</td>\n",
       "      <td>-3.16250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.59120</td>\n",
       "      <td>3.0129</td>\n",
       "      <td>0.72888</td>\n",
       "      <td>0.56421</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.09220</td>\n",
       "      <td>-6.8100</td>\n",
       "      <td>8.46360</td>\n",
       "      <td>-0.60216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.20320</td>\n",
       "      <td>5.7588</td>\n",
       "      <td>-0.75345</td>\n",
       "      <td>-0.61251</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.53560</td>\n",
       "      <td>9.1772</td>\n",
       "      <td>-2.27180</td>\n",
       "      <td>-0.73535</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.22470</td>\n",
       "      <td>8.7779</td>\n",
       "      <td>-2.21350</td>\n",
       "      <td>-0.80647</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        x1      x2       x3       x4  cls\n",
       "0  4.54590  8.1674 -2.45860 -1.46210    0\n",
       "1  3.86600 -2.6383  1.92420  0.10645    0\n",
       "2  3.45660  9.5228 -4.01120 -3.59440    0\n",
       "3  0.32924 -4.4552  4.57180 -0.98880    0\n",
       "4  4.36840  9.6718 -3.96060 -3.16250    0\n",
       "5  3.59120  3.0129  0.72888  0.56421    0\n",
       "6  2.09220 -6.8100  8.46360 -0.60216    0\n",
       "7  3.20320  5.7588 -0.75345 -0.61251    0\n",
       "8  1.53560  9.1772 -2.27180 -0.73535    0\n",
       "9  1.22470  8.7779 -2.21350 -0.80647    0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dados_autent_bancaria.csv')\n",
    "df.columns =['x1','x2','x3','x4','cls']\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>cls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1371.000000</td>\n",
       "      <td>1371.000000</td>\n",
       "      <td>1371.000000</td>\n",
       "      <td>1371.000000</td>\n",
       "      <td>1371.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>53.894598</td>\n",
       "      <td>58.711732</td>\n",
       "      <td>28.805627</td>\n",
       "      <td>66.886716</td>\n",
       "      <td>44.493071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>20.498413</td>\n",
       "      <td>21.958557</td>\n",
       "      <td>18.567234</td>\n",
       "      <td>19.110209</td>\n",
       "      <td>49.713946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>37.985419</td>\n",
       "      <td>45.133528</td>\n",
       "      <td>16.080083</td>\n",
       "      <td>55.749839</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>54.358292</td>\n",
       "      <td>60.193379</td>\n",
       "      <td>25.428005</td>\n",
       "      <td>72.392864</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>71.081136</td>\n",
       "      <td>77.030612</td>\n",
       "      <td>36.477481</td>\n",
       "      <td>81.317094</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                x1           x2           x3           x4          cls\n",
       "count  1371.000000  1371.000000  1371.000000  1371.000000  1371.000000\n",
       "mean     53.894598    58.711732    28.805627    66.886716    44.493071\n",
       "std      20.498413    21.958557    18.567234    19.110209    49.713946\n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000\n",
       "25%      37.985419    45.133528    16.080083    55.749839     0.000000\n",
       "50%      54.358292    60.193379    25.428005    72.392864     0.000000\n",
       "75%      71.081136    77.030612    36.477481    81.317094   100.000000\n",
       "max     100.000000   100.000000   100.000000   100.000000   100.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        if df[feature_name].dtypes != \"object\":\n",
    "            max_value = df[feature_name].max()\n",
    "            min_value = df[feature_name].min()\n",
    "            result[feature_name] = ((df[feature_name] - min_value) / (max_value - min_value))*100\n",
    "    return result\n",
    "\n",
    "\n",
    "df_normalized = normalize(df)\n",
    "\n",
    "df_normalized.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInput = df[[\"x1\",\"x2\",\"x3\",\"x4\"]]\n",
    "dfOutput = df[\"cls\"]\n",
    "\n",
    "dfInput_train, dfInput_test, dfOutput_train, dfOutput_test = train_test_split(dfInput, dfOutput, shuffle=True, test_size=0.25, random_state=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que ja normalizei e dividi o dataset comeco a parametrizar o Perceptron e o MLPClassifier.\n",
    "\n",
    "perceptron - sklearn.linaer_model import Perceptron\n",
    "Multi-layer Perceptron classifier - This model optimizes the log-loss function using LBFGS or stochastic gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 46.84, NNZs: 4, Bias: 28.000000, T: 1028, Avg. loss: 1.576500\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 55.53, NNZs: 4, Bias: 36.000000, T: 2056, Avg. loss: 0.716679\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 64.14, NNZs: 4, Bias: 40.000000, T: 3084, Avg. loss: 1.101396\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 68.74, NNZs: 4, Bias: 47.000000, T: 4112, Avg. loss: 0.805822\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 75.91, NNZs: 4, Bias: 47.000000, T: 5140, Avg. loss: 0.556079\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 79.50, NNZs: 4, Bias: 53.000000, T: 6168, Avg. loss: 1.088156\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 80.78, NNZs: 4, Bias: 55.000000, T: 7196, Avg. loss: 0.605769\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 84.12, NNZs: 4, Bias: 57.000000, T: 8224, Avg. loss: 0.475273\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 85.82, NNZs: 4, Bias: 64.000000, T: 9252, Avg. loss: 0.637956\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 90.47, NNZs: 4, Bias: 65.000000, T: 10280, Avg. loss: 0.513904\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 95.30, NNZs: 4, Bias: 66.000000, T: 11308, Avg. loss: 0.719511\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 96.71, NNZs: 4, Bias: 70.000000, T: 12336, Avg. loss: 0.658810\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 103.93, NNZs: 4, Bias: 69.000000, T: 13364, Avg. loss: 1.045559\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 13 epochs took 0.01 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9873540856031129"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "clf = Perceptron(penalty=\"l1\",validation_fraction=0.2, verbose=1)\n",
    "clf.fit(dfInput_train,dfOutput_train)\n",
    "clf.score(dfInput_train,dfOutput_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 51.44, NNZs: 4, Bias: 23.358082, T: 959, Avg. loss: 1.509367\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 57.06, NNZs: 4, Bias: 37.603748, T: 1918, Avg. loss: 1.103148\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 60.80, NNZs: 4, Bias: 44.007124, T: 2877, Avg. loss: 0.927815\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 59.17, NNZs: 4, Bias: 46.180263, T: 3836, Avg. loss: 0.697028\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 61.27, NNZs: 4, Bias: 47.885017, T: 4795, Avg. loss: 0.758672\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 65.81, NNZs: 4, Bias: 51.144726, T: 5754, Avg. loss: 0.821050\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 64.93, NNZs: 4, Bias: 47.716229, T: 6713, Avg. loss: 0.591920\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 65.29, NNZs: 4, Bias: 48.083342, T: 7672, Avg. loss: 0.881018\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 68.23, NNZs: 4, Bias: 48.450455, T: 8631, Avg. loss: 0.907817\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 66.81, NNZs: 4, Bias: 54.853831, T: 9590, Avg. loss: 0.865027\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 70.95, NNZs: 4, Bias: 57.896226, T: 10549, Avg. loss: 1.039462\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 71.43, NNZs: 4, Bias: 60.503993, T: 11508, Avg. loss: 0.715577\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 12 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 47.13, NNZs: 4, Bias: 22.237755, T: 959, Avg. loss: 1.677620\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 50.21, NNZs: 4, Bias: 36.048793, T: 1918, Avg. loss: 1.151035\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 58.98, NNZs: 4, Bias: 40.646142, T: 2877, Avg. loss: 1.043836\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 60.58, NNZs: 4, Bias: 45.494564, T: 3836, Avg. loss: 0.724441\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 65.35, NNZs: 4, Bias: 50.777613, T: 4795, Avg. loss: 1.151199\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 66.70, NNZs: 4, Bias: 53.786250, T: 5754, Avg. loss: 1.296616\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 66.77, NNZs: 4, Bias: 53.033036, T: 6713, Avg. loss: 0.737795\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 69.32, NNZs: 4, Bias: 50.473795, T: 7672, Avg. loss: 0.877821\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 68.52, NNZs: 4, Bias: 53.733504, T: 8631, Avg. loss: 0.879564\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 9 epochs took 0.00 seconds\n",
      "-- Epoch 1\n",
      "Norm: 39.00, NNZs: 4, Bias: 22.271512, T: 959, Avg. loss: 1.362894\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 52.74, NNZs: 4, Bias: 34.711152, T: 1918, Avg. loss: 1.157992\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 58.55, NNZs: 4, Bias: 40.211515, T: 2877, Avg. loss: 1.139737\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 68.85, NNZs: 4, Bias: 46.146505, T: 3836, Avg. loss: 0.959830\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 69.07, NNZs: 4, Bias: 45.610605, T: 4795, Avg. loss: 0.774652\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 71.74, NNZs: 4, Bias: 50.893654, T: 5754, Avg. loss: 0.736308\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 69.03, NNZs: 4, Bias: 53.284107, T: 6713, Avg. loss: 0.885387\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 72.20, NNZs: 4, Bias: 51.627880, T: 7672, Avg. loss: 0.761349\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 68.78, NNZs: 4, Bias: 51.777679, T: 8631, Avg. loss: 0.924542\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 70.73, NNZs: 4, Bias: 54.820074, T: 9590, Avg. loss: 0.783249\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 75.68, NNZs: 4, Bias: 58.297096, T: 10549, Avg. loss: 1.014793\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 11 epochs took 0.00 seconds\n",
      "-- Epoch 1\n",
      "Norm: 39.82, NNZs: 4, Bias: 20.682800, T: 959, Avg. loss: 1.543229\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 46.56, NNZs: 4, Bias: 34.928466, T: 1918, Avg. loss: 1.383166\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 51.40, NNZs: 4, Bias: 40.211515, T: 2877, Avg. loss: 0.981356\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 57.24, NNZs: 4, Bias: 46.614891, T: 3836, Avg. loss: 0.971834\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 59.27, NNZs: 4, Bias: 47.199318, T: 4795, Avg. loss: 0.752978\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 62.66, NNZs: 4, Bias: 48.686757, T: 5754, Avg. loss: 0.887190\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 63.30, NNZs: 4, Bias: 51.294525, T: 6713, Avg. loss: 0.712135\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 63.13, NNZs: 4, Bias: 49.638297, T: 7672, Avg. loss: 0.701443\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 65.28, NNZs: 4, Bias: 52.680692, T: 8631, Avg. loss: 1.231796\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 65.21, NNZs: 4, Bias: 54.168132, T: 9590, Avg. loss: 0.569958\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 68.35, NNZs: 4, Bias: 59.668495, T: 10549, Avg. loss: 1.195238\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 71.67, NNZs: 4, Bias: 58.229581, T: 11508, Avg. loss: 0.783972\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 69.15, NNZs: 4, Bias: 61.740362, T: 12467, Avg. loss: 0.684910\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 66.64, NNZs: 4, Bias: 64.782756, T: 13426, Avg. loss: 0.830612\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 68.65, NNZs: 4, Bias: 62.440830, T: 14385, Avg. loss: 0.683905\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 15 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 49.48, NNZs: 4, Bias: 24.227337, T: 959, Avg. loss: 1.874024\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 49.88, NNZs: 4, Bias: 32.219426, T: 1918, Avg. loss: 0.842112\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 54.67, NNZs: 4, Bias: 37.502475, T: 2877, Avg. loss: 0.867388\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 56.11, NNZs: 4, Bias: 42.785524, T: 3836, Avg. loss: 1.062298\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 55.02, NNZs: 4, Bias: 47.199318, T: 4795, Avg. loss: 0.602727\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 60.86, NNZs: 4, Bias: 50.893654, T: 5754, Avg. loss: 1.084815\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 65.73, NNZs: 4, Bias: 51.009696, T: 6713, Avg. loss: 0.908359\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 68.38, NNZs: 4, Bias: 49.570782, T: 7672, Avg. loss: 0.587926\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 68.30, NNZs: 4, Bias: 52.830491, T: 8631, Avg. loss: 1.343122\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 66.95, NNZs: 4, Bias: 53.197604, T: 9590, Avg. loss: 0.666692\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 67.81, NNZs: 4, Bias: 54.685044, T: 10549, Avg. loss: 0.984866\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 70.28, NNZs: 4, Bias: 57.075497, T: 11508, Avg. loss: 0.695800\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 70.79, NNZs: 4, Bias: 58.562937, T: 12467, Avg. loss: 0.808840\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 13 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 43.97, NNZs: 4, Bias: 25.598736, T: 959, Avg. loss: 1.815094\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 48.98, NNZs: 4, Bias: 31.567484, T: 1918, Avg. loss: 1.050448\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 54.01, NNZs: 4, Bias: 33.924180, T: 2877, Avg. loss: 1.348453\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 55.26, NNZs: 4, Bias: 35.628934, T: 3836, Avg. loss: 0.835433\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 61.82, NNZs: 4, Bias: 44.272964, T: 4795, Avg. loss: 0.966490\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 63.70, NNZs: 4, Bias: 48.435686, T: 5754, Avg. loss: 1.146445\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 67.77, NNZs: 4, Bias: 49.454741, T: 6713, Avg. loss: 0.878526\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 68.27, NNZs: 4, Bias: 48.918840, T: 7672, Avg. loss: 0.775532\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 66.93, NNZs: 4, Bias: 50.188966, T: 8631, Avg. loss: 0.990530\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 70.68, NNZs: 4, Bias: 50.556079, T: 9590, Avg. loss: 0.702708\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 74.46, NNZs: 4, Bias: 54.284173, T: 10549, Avg. loss: 0.932999\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 69.22, NNZs: 4, Bias: 54.868600, T: 11508, Avg. loss: 0.908708\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 70.16, NNZs: 4, Bias: 55.235713, T: 12467, Avg. loss: 0.774435\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 69.72, NNZs: 4, Bias: 51.807217, T: 13426, Avg. loss: 0.597416\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 70.50, NNZs: 4, Bias: 52.391643, T: 14385, Avg. loss: 0.944279\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 67.71, NNZs: 4, Bias: 54.347469, T: 15344, Avg. loss: 0.554119\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 68.95, NNZs: 4, Bias: 55.149209, T: 16303, Avg. loss: 0.817818\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 68.15, NNZs: 4, Bias: 53.058354, T: 17262, Avg. loss: 0.819607\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 72.55, NNZs: 4, Bias: 58.341403, T: 18221, Avg. loss: 1.057400\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 73.64, NNZs: 4, Bias: 54.444521, T: 19180, Avg. loss: 0.718965\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 70.04, NNZs: 4, Bias: 57.737988, T: 20139, Avg. loss: 0.803873\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 21 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 41.95, NNZs: 4, Bias: 27.873148, T: 959, Avg. loss: 1.301030\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 48.43, NNZs: 4, Bias: 35.865236, T: 1918, Avg. loss: 0.892466\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 55.27, NNZs: 4, Bias: 42.485926, T: 2877, Avg. loss: 0.944120\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 55.19, NNZs: 4, Bias: 47.117034, T: 3836, Avg. loss: 0.800052\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 58.65, NNZs: 4, Bias: 48.169846, T: 4795, Avg. loss: 0.690182\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 63.89, NNZs: 4, Bias: 54.573222, T: 5754, Avg. loss: 0.967743\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 67.99, NNZs: 4, Bias: 50.459026, T: 6713, Avg. loss: 0.643699\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 64.73, NNZs: 4, Bias: 56.678846, T: 7672, Avg. loss: 0.502727\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 67.35, NNZs: 4, Bias: 57.263273, T: 8631, Avg. loss: 0.822609\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 71.84, NNZs: 4, Bias: 56.727373, T: 9590, Avg. loss: 0.666713\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 70.67, NNZs: 4, Bias: 60.020839, T: 10549, Avg. loss: 0.806498\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 66.97, NNZs: 4, Bias: 60.639023, T: 11508, Avg. loss: 0.530047\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 67.27, NNZs: 4, Bias: 63.029476, T: 12467, Avg. loss: 0.707457\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 13 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 36.34, NNZs: 4, Bias: 21.368499, T: 959, Avg. loss: 1.292917\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 50.44, NNZs: 4, Bias: 28.206503, T: 1918, Avg. loss: 0.787084\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 58.45, NNZs: 4, Bias: 40.211515, T: 2877, Avg. loss: 1.319300\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 61.20, NNZs: 4, Bias: 38.772601, T: 3836, Avg. loss: 0.759029\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 59.09, NNZs: 4, Bias: 44.524035, T: 4795, Avg. loss: 0.787516\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 58.59, NNZs: 4, Bias: 44.891148, T: 5754, Avg. loss: 0.620161\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 63.23, NNZs: 4, Bias: 44.137934, T: 6713, Avg. loss: 1.022878\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 64.42, NNZs: 4, Bias: 46.745701, T: 7672, Avg. loss: 0.704293\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 70.26, NNZs: 4, Bias: 52.028750, T: 8631, Avg. loss: 1.035025\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 73.12, NNZs: 4, Bias: 52.830491, T: 9590, Avg. loss: 0.822547\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 71.64, NNZs: 4, Bias: 58.581925, T: 10549, Avg. loss: 1.196611\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 11 epochs took 0.00 seconds\n",
      "-- Epoch 1\n",
      "Norm: 34.65, NNZs: 4, Bias: 23.104579, T: 960, Avg. loss: 1.311494\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 48.88, NNZs: 4, Bias: 31.448777, T: 1920, Avg. loss: 1.183062\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 55.44, NNZs: 4, Bias: 43.193868, T: 2880, Avg. loss: 0.936999\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 59.04, NNZs: 4, Bias: 41.253210, T: 3840, Avg. loss: 1.152148\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 59.58, NNZs: 4, Bias: 43.832326, T: 4800, Avg. loss: 1.075979\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 59.95, NNZs: 4, Bias: 45.507487, T: 5760, Avg. loss: 0.991841\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 61.40, NNZs: 4, Bias: 44.729959, T: 6720, Avg. loss: 0.850596\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 62.63, NNZs: 4, Bias: 49.976690, T: 7680, Avg. loss: 1.437062\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 63.29, NNZs: 4, Bias: 51.866777, T: 8640, Avg. loss: 0.908679\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 69.29, NNZs: 4, Bias: 56.639406, T: 9600, Avg. loss: 0.903405\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 64.04, NNZs: 4, Bias: 54.313144, T: 10560, Avg. loss: 0.840323\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 67.63, NNZs: 4, Bias: 53.276441, T: 11520, Avg. loss: 1.242191\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 71.52, NNZs: 4, Bias: 53.143692, T: 12480, Avg. loss: 0.978164\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 70.76, NNZs: 4, Bias: 58.175497, T: 13440, Avg. loss: 0.711827\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 74.49, NNZs: 4, Bias: 58.946703, T: 14400, Avg. loss: 0.860180\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 71.66, NNZs: 4, Bias: 58.169175, T: 15360, Avg. loss: 0.927596\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 71.86, NNZs: 4, Bias: 60.318439, T: 16320, Avg. loss: 0.545928\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 67.73, NNZs: 4, Bias: 61.778673, T: 17280, Avg. loss: 0.784617\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 70.45, NNZs: 4, Bias: 58.548457, T: 18240, Avg. loss: 0.545574\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 72.73, NNZs: 4, Bias: 58.200782, T: 19200, Avg. loss: 0.950967\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 72.39, NNZs: 4, Bias: 60.090870, T: 20160, Avg. loss: 1.067579\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 69.42, NNZs: 4, Bias: 57.979535, T: 21120, Avg. loss: 0.767824\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 22 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 33.63, NNZs: 4, Bias: 20.651890, T: 960, Avg. loss: 1.190922\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 47.15, NNZs: 4, Bias: 30.114970, T: 1920, Avg. loss: 1.024406\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 55.76, NNZs: 4, Bias: 36.695508, T: 2880, Avg. loss: 1.070943\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 61.17, NNZs: 4, Bias: 39.704476, T: 3840, Avg. loss: 0.900857\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 61.01, NNZs: 4, Bias: 44.736281, T: 4800, Avg. loss: 0.966461\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 66.27, NNZs: 4, Bias: 48.864130, T: 5760, Avg. loss: 0.831149\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 67.04, NNZs: 4, Bias: 53.895935, T: 6720, Avg. loss: 1.123804\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 65.58, NNZs: 4, Bias: 54.237288, T: 7680, Avg. loss: 0.755410\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 65.25, NNZs: 4, Bias: 55.697523, T: 8640, Avg. loss: 0.852537\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 69.31, NNZs: 4, Bias: 57.587610, T: 9600, Avg. loss: 0.878387\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 72.12, NNZs: 4, Bias: 58.143890, T: 10560, Avg. loss: 0.837273\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 73.15, NNZs: 4, Bias: 54.654498, T: 11520, Avg. loss: 1.119519\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 69.26, NNZs: 4, Bias: 56.588835, T: 12480, Avg. loss: 0.790672\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 13 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 37.76, NNZs: 4, Bias: 21.985698, T: 960, Avg. loss: 1.108997\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 49.35, NNZs: 4, Bias: 31.018925, T: 1920, Avg. loss: 1.037852\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 52.10, NNZs: 4, Bias: 36.094978, T: 2880, Avg. loss: 0.471192\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 60.54, NNZs: 4, Bias: 42.675517, T: 3840, Avg. loss: 0.965732\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 57.61, NNZs: 4, Bias: 44.135751, T: 4800, Avg. loss: 0.810329\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 62.95, NNZs: 4, Bias: 43.573150, T: 5760, Avg. loss: 0.637245\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 59.59, NNZs: 4, Bias: 51.101892, T: 6720, Avg. loss: 0.839415\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 63.56, NNZs: 4, Bias: 50.754218, T: 7680, Avg. loss: 0.945749\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 8 epochs took 0.00 seconds\n",
      "-- Epoch 1\n",
      "Norm: 41.13, NNZs: 4, Bias: 24.438386, T: 960, Avg. loss: 1.010721\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 47.32, NNZs: 4, Bias: 33.515863, T: 1920, Avg. loss: 0.709415\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 54.96, NNZs: 4, Bias: 43.238118, T: 2880, Avg. loss: 1.081427\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 61.02, NNZs: 4, Bias: 47.151041, T: 3840, Avg. loss: 0.781188\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 62.39, NNZs: 4, Bias: 49.041128, T: 4800, Avg. loss: 0.961091\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 61.89, NNZs: 4, Bias: 52.524199, T: 5760, Avg. loss: 0.624506\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 63.30, NNZs: 4, Bias: 53.080479, T: 6720, Avg. loss: 0.924852\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 72.35, NNZs: 4, Bias: 53.377583, T: 7680, Avg. loss: 0.853909\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 69.46, NNZs: 4, Bias: 55.311920, T: 8640, Avg. loss: 0.799252\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 67.92, NNZs: 4, Bias: 56.127375, T: 9600, Avg. loss: 0.505184\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 68.09, NNZs: 4, Bias: 63.396942, T: 10560, Avg. loss: 0.800905\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 66.45, NNZs: 4, Bias: 61.974636, T: 11520, Avg. loss: 0.678769\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 68.52, NNZs: 4, Bias: 60.508079, T: 12480, Avg. loss: 0.676481\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 69.71, NNZs: 4, Bias: 62.613093, T: 13440, Avg. loss: 0.781380\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 71.65, NNZs: 4, Bias: 63.169373, T: 14400, Avg. loss: 0.782782\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 15 epochs took 0.01 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 39.40, NNZs: 4, Bias: 23.749358, T: 960, Avg. loss: 1.331635\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 46.22, NNZs: 4, Bias: 35.235273, T: 1920, Avg. loss: 1.548895\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 55.94, NNZs: 4, Bias: 43.838647, T: 2880, Avg. loss: 0.766739\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 62.53, NNZs: 4, Bias: 45.039706, T: 3840, Avg. loss: 0.892798\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 62.79, NNZs: 4, Bias: 49.167556, T: 4800, Avg. loss: 1.351968\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 62.66, NNZs: 4, Bias: 51.961598, T: 5760, Avg. loss: 0.515110\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 70.01, NNZs: 4, Bias: 56.045198, T: 6720, Avg. loss: 1.362187\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 68.54, NNZs: 4, Bias: 60.647149, T: 7680, Avg. loss: 0.649885\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 70.97, NNZs: 4, Bias: 61.418356, T: 8640, Avg. loss: 0.951044\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 77.57, NNZs: 4, Bias: 61.285607, T: 9600, Avg. loss: 0.765740\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 74.87, NNZs: 4, Bias: 68.081071, T: 10560, Avg. loss: 1.138096\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 11 epochs took 0.01 seconds\n",
      "-- Epoch 1\n",
      "Norm: 45.57, NNZs: 4, Bias: 21.037494, T: 960, Avg. loss: 1.475433\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 54.30, NNZs: 4, Bias: 30.715499, T: 1920, Avg. loss: 1.277076\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 56.30, NNZs: 4, Bias: 40.696930, T: 2880, Avg. loss: 0.741171\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 61.28, NNZs: 4, Bias: 48.396349, T: 3840, Avg. loss: 0.887059\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 62.11, NNZs: 4, Bias: 52.524199, T: 4800, Avg. loss: 1.148455\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 64.95, NNZs: 4, Bias: 51.961598, T: 5760, Avg. loss: 0.576605\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 66.66, NNZs: 4, Bias: 56.993402, T: 6720, Avg. loss: 1.172578\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 67.19, NNZs: 4, Bias: 59.142665, T: 7680, Avg. loss: 0.525135\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 68.68, NNZs: 4, Bias: 59.913871, T: 8640, Avg. loss: 0.717609\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 72.47, NNZs: 4, Bias: 61.374106, T: 9600, Avg. loss: 0.815915\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 68.92, NNZs: 4, Bias: 61.715460, T: 10560, Avg. loss: 1.003867\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 67.49, NNZs: 4, Bias: 60.937932, T: 11520, Avg. loss: 1.211650\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 69.01, NNZs: 4, Bias: 57.233614, T: 12480, Avg. loss: 0.831231\n",
      "Total training time: 0.02 seconds.\n",
      "Convergence after 13 epochs took 0.02 seconds\n",
      "-- Epoch 1\n",
      "Norm: 44.49, NNZs: 4, Bias: 23.644157, T: 960, Avg. loss: 1.746357\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 52.88, NNZs: 4, Bias: 26.932752, T: 1920, Avg. loss: 0.844707\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 60.95, NNZs: 4, Bias: 34.049610, T: 2880, Avg. loss: 0.888525\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 63.64, NNZs: 4, Bias: 38.240461, T: 3840, Avg. loss: 1.071654\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 61.28, NNZs: 4, Bias: 45.357319, T: 4800, Avg. loss: 0.953633\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 62.60, NNZs: 4, Bias: 47.988195, T: 5760, Avg. loss: 0.736169\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 70.20, NNZs: 4, Bias: 55.079755, T: 6720, Avg. loss: 1.014293\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 65.61, NNZs: 4, Bias: 59.295903, T: 7680, Avg. loss: 0.614265\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 68.98, NNZs: 4, Bias: 53.612536, T: 8640, Avg. loss: 0.880203\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 62.54, NNZs: 4, Bias: 60.973930, T: 9600, Avg. loss: 0.700791\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 64.99, NNZs: 4, Bias: 58.655049, T: 10560, Avg. loss: 0.889578\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 63.32, NNZs: 4, Bias: 57.921439, T: 11520, Avg. loss: 0.727395\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 68.44, NNZs: 4, Bias: 57.162532, T: 12480, Avg. loss: 0.894945\n",
      "Total training time: 0.02 seconds.\n",
      "Convergence after 13 epochs took 0.02 seconds\n",
      "[0.91304348 1.         1.         1.         0.98550725 0.98550725\n",
      " 0.94202899 0.98550725 1.         0.98529412 0.97058824 0.97058824\n",
      " 0.98529412 0.98529412 1.        ]\n",
      "Média Perceptron: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "cross_perceptron = cross_val_score(Perceptron(penalty=\"l2\",max_iter=200, class_weight=\"balanced\", validation_fraction=0.2, verbose=1),dfInput_train,dfOutput_train,verbose=1, cv=15)\n",
    "print(cross_perceptron)\n",
    "print(\"Média Perceptron: {0:.2f}\".format(cross_perceptron.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora comeco a utilizar MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo com função logistica de ativação: 0.991\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mlp_logistic = MLPClassifier(hidden_layer_sizes=(), activation=\"relu\", solver=\"sgd\", validation_fraction=0.2)\n",
    "mlp_logistic.fit(dfInput_train,dfOutput_train)\n",
    "dfOutput_predicted_logistic = mlp_logistic.predict(dfInput_test)\n",
    "print(\"Modelo com função logistica de ativação: {0:.3f}\".format(accuracy_score(dfOutput_test, dfOutput_predicted_logistic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo com função logistica de ativação: 1.000\n"
     ]
    }
   ],
   "source": [
    "mlp_logistic = MLPClassifier(hidden_layer_sizes=(4), batch_size=,solver=\"sgd\", max_iter=500,validation_fraction=0.15,activation=\"logistic\", learning_rate_init=0.001)\n",
    "mlp_logistic.fit(dfInput_train,dfOutput_train)\n",
    "dfOutput_predicted_logistic = mlp_logistic.predict(dfInput_test)\n",
    "print(\"Modelo com função logistica de ativação: {0:.3f}\".format(accuracy_score(dfOutput_test, dfOutput_predicted_logistic)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_logistic = MLPClassifier(hidden_layer_sizes=(6),solver=\"sgd\",max_iter=500,activation=\"logistic\")\n",
    "mlp_logistic.fit(dfInput_train,dfOutput_train)\n",
    "dfOutput_predicted_logistic = mlp_logistic.predict(dfInput_test)\n",
    "print(\"Modelo com função logistica de ativação: {0:.3f}\".format(accuracy_score(dfOutput_test, dfOutput_predicted_logistic)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
